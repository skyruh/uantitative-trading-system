{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM Training on Kaggle\n",
    "\n",
    "This notebook trains LSTM models for stock price prediction using Kaggle's GPU resources.\n",
    "\n",
    "## Setup\n",
    "- **GPU**: T4/P100 recommended\n",
    "- **Time**: 8-12 hours for all stocks\n",
    "- **Batch Size**: 256-512 (optimized for Kaggle)\n",
    "- **Output**: Trained models saved to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Setup Environment\n",
    "!git clone https://github.com/yourusername/trading-system.git\n",
    "%cd trading-system\n",
    "!pip install -r requirements.txt\n",
    "\n",
    "# Check GPU availability\n",
    "import tensorflow as tf\n",
    "print(\"GPU Available: \", tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "# Configure GPU memory growth\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(f\"Configured {len(gpus)} GPU(s) for memory growth\")\n",
    "    except RuntimeError as e:\n",
    "        print(f\"GPU configuration error: {e}\")\n",
    "\n",
    "# Enable mixed precision for faster training\n",
    "policy = tf.keras.mixed_precision.Policy('mixed_float16')\n",
    "tf.keras.mixed_precision.set_global_policy(policy)\n",
    "print(\"Mixed precision enabled\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Mount Google Drive and Setup Paths\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Add source code to path\n",
    "sys.path.append('/kaggle/working/trading-system/src')\n",
    "\n",
    "# Define paths\n",
    "data_path = '/content/drive/MyDrive/TradingSystem/data/processed'\n",
    "models_path = '/content/drive/MyDrive/TradingSystem/models/lstm'\n",
    "logs_path = '/content/drive/MyDrive/TradingSystem/logs'\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(models_path, exist_ok=True)\n",
    "os.makedirs(logs_path, exist_ok=True)\n",
    "\n",
    "print(f\"Data path: {data_path}\")\n",
    "print(f\"Models path: {models_path}\")\n",
    "print(f\"Logs path: {logs_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Load Processed Data\n",
    "# Check available processed files\n",
    "if os.path.exists(data_path):\n",
    "    processed_files = [f for f in os.listdir(data_path) if f.endswith('_processed.csv')]\n",
    "    print(f\"Found {len(processed_files)} processed files\")\n",
    "    \n",
    "    # Display first few files\n",
    "    for i, file in enumerate(processed_files[:10]):\n",
    "        print(f\"{i+1}. {file}\")\n",
    "    \n",
    "    if len(processed_files) > 10:\n",
    "        print(f\"... and {len(processed_files) - 10} more files\")\n",
    "else:\n",
    "    print(f\"Data path does not exist: {data_path}\")\n",
    "    print(\"Please upload your processed data to Google Drive first\")\n",
    "\n",
    "# Load data for training\n",
    "training_data = {}\n",
    "failed_loads = []\n",
    "\n",
    "for file in processed_files:\n",
    "    try:\n",
    "        symbol = file.replace('_processed.csv', '')\n",
    "        df = pd.read_csv(os.path.join(data_path, file))\n",
    "        \n",
    "        if len(df) > 100:  # Only include stocks with sufficient data\n",
    "            training_data[symbol] = df\n",
    "            print(f\"✓ Loaded {symbol}: {len(df)} records\")\n",
    "        else:\n",
    "            print(f\"⚠ Skipped {symbol}: insufficient data ({len(df)} records)\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        failed_loads.append((file, str(e)))\n",
    "        print(f\"✗ Failed to load {file}: {e}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(training_data)} stocks for training\")\n",
    "if failed_loads:\n",
    "    print(f\"Failed to load {len(failed_loads)} files\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4: Import Trading System Modules\n",
    "try:\n",
    "    from models.lstm_model import LSTMModel\n",
    "    from models.lstm_trainer import LSTMTrainer\n",
    "    print(\"✓ Successfully imported trading system modules\")\n",
    "except ImportError as e:\n",
    "    print(f\"✗ Import error: {e}\")\n",
    "    print(\"Falling back to simplified LSTM implementation\")\n",
    "    \n",
    "    # Simplified LSTM implementation for Kaggle\n",
    "    from tensorflow.keras.models import Sequential\n",
    "    from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    def create_lstm_model(input_shape):\n",
    "        \"\"\"Create optimized LSTM model for Kaggle training\"\"\"\n",
    "        model = Sequential([\n",
    "            LSTM(128, return_sequences=True, input_shape=input_shape),\n",
    "            Dropout(0.2),\n",
    "            LSTM(64, return_sequences=True),\n",
    "            Dropout(0.2),\n",
    "            LSTM(32),\n",
    "            Dropout(0.2),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1, activation='sigmoid')\n",
    "        ])\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='adam',\n",
    "            loss='binary_crossentropy',\n",
    "            metrics=['accuracy']\n",
    "        )\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    print(\"✓ Simplified LSTM implementation ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Training Configuration\n",
    "# Kaggle-optimized training parameters\n",
    "TRAINING_CONFIG = {\n",
    "    'batch_size': 256,  # Optimized for Kaggle T4/P100\n",
    "    'epochs': 50,       # Reduced for faster training\n",
    "    'sequence_length': 60,\n",
    "    'validation_split': 0.2,\n",
    "    'patience': 10,     # Early stopping patience\n",
    "    'lr_patience': 5,   # Learning rate reduction patience\n",
    "}\n",
    "\n",
    "# Detect GPU type and adjust batch size\n",
    "gpu_name = tf.config.experimental.get_device_details(gpus[0])['device_name'] if gpus else 'CPU'\n",
    "print(f\"Detected GPU: {gpu_name}\")\n",
    "\n",
    "if 'V100' in gpu_name:\n",
    "    TRAINING_CONFIG['batch_size'] = 512\n",
    "    print(\"Optimized for V100: batch_size = 512\")\n",
    "elif 'P100' in gpu_name:\n",
    "    TRAINING_CONFIG['batch_size'] = 384\n",
    "    print(\"Optimized for P100: batch_size = 384\")\n",
    "elif 'T4' in gpu_name:\n",
    "    TRAINING_CONFIG['batch_size'] = 256\n",
    "    print(\"Optimized for T4: batch_size = 256\")\n",
    "else:\n",
    "    TRAINING_CONFIG['batch_size'] = 128\n",
    "    print(\"Conservative batch_size = 128\")\n",
    "\n",
    "print(f\"Training configuration: {TRAINING_CONFIG}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Data Preparation Function\n",
    "def prepare_lstm_data(df, sequence_length=60):\n",
    "    \"\"\"Prepare data for LSTM training\"\"\"\n",
    "    try:\n",
    "        # Select features (adjust based on your processed data columns)\n",
    "        feature_columns = [\n",
    "            'close', 'volume', 'rsi_14', 'sma_50', 'sma_200',\n",
    "            'macd', 'macd_signal', 'bb_upper', 'bb_lower', 'atr_14'\n",
    "        ]\n",
    "        \n",
    "        # Filter available columns\n",
    "        available_features = [col for col in feature_columns if col in df.columns]\n",
    "        \n",
    "        if not available_features:\n",
    "            print(f\"No feature columns found. Available columns: {df.columns.tolist()}\")\n",
    "            return None, None, None, None\n",
    "        \n",
    "        # Prepare features and target\n",
    "        features = df[available_features].values\n",
    "        target = df['target'].values if 'target' in df.columns else (df['close'].shift(-1) > df['close']).astype(int).values[:-1]\n",
    "        \n",
    "        # Scale features\n",
    "        scaler = MinMaxScaler()\n",
    "        features_scaled = scaler.fit_transform(features)\n",
    "        \n",
    "        # Create sequences\n",
    "        X, y = [], []\n",
    "        for i in range(sequence_length, len(features_scaled) - 1):\n",
    "            X.append(features_scaled[i-sequence_length:i])\n",
    "            y.append(target[i])\n",
    "        \n",
    "        X, y = np.array(X), np.array(y)\n",
    "        \n",
    "        # Train-test split\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing data: {e}\")\n",
    "        return None, None, None, None\n",
    "\n",
    "print(\"Data preparation function ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Training Loop\n",
    "training_results = {}\n",
    "training_start_time = datetime.now()\n",
    "\n",
    "print(f\"Starting LSTM training for {len(training_data)} stocks...\")\n",
    "print(f\"Training started at: {training_start_time}\")\n",
    "\n",
    "for i, (symbol, df) in enumerate(training_data.items()):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Training {i+1}/{len(training_data)}: {symbol}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    try:\n",
    "        # Prepare data\n",
    "        X_train, X_test, y_train, y_test = prepare_lstm_data(df, TRAINING_CONFIG['sequence_length'])\n",
    "        \n",
    "        if X_train is None:\n",
    "            print(f\"⚠ Skipping {symbol}: data preparation failed\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Data shape - Train: {X_train.shape}, Test: {X_test.shape}\")\n",
    "        \n",
    "        # Create model\n",
    "        model = create_lstm_model((X_train.shape[1], X_train.shape[2]))\n",
    "        \n",
    "        # Setup callbacks\n",
    "        callbacks = [\n",
    "            EarlyStopping(\n",
    "                monitor='val_loss',\n",
    "                patience=TRAINING_CONFIG['patience'],\n",
    "                restore_best_weights=True,\n",
    "                verbose=1\n",
    "            ),\n",
    "            ReduceLROnPlateau(\n",
    "                monitor='val_loss',\n",
    "                patience=TRAINING_CONFIG['lr_patience'],\n",
    "                factor=0.5,\n",
    "                verbose=1\n",
    "            )\n",
    "        ]\n",
    "        \n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train, y_train,\n",
    "            batch_size=TRAINING_CONFIG['batch_size'],\n",
    "            epochs=TRAINING_CONFIG['epochs'],\n",
    "            validation_split=TRAINING_CONFIG['validation_split'],\n",
    "            callbacks=callbacks,\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        test_loss, test_accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
    "        \n",
    "        # Save model\n",
    "        model_path = os.path.join(models_path, f'{symbol}_model.keras')\n",
    "        model.save(model_path)\n",
    "        \n",
    "        # Store results\n",
    "        training_results[symbol] = {\n",
    "            'test_accuracy': float(test_accuracy),\n",
    "            'test_loss': float(test_loss),\n",
    "            'epochs_trained': len(history.history['loss']),\n",
    "            'best_val_accuracy': float(max(history.history['val_accuracy'])),\n",
    "            'model_path': model_path\n",
    "        }\n",
    "        \n",
    "        print(f\"✓ {symbol} - Test Accuracy: {test_accuracy:.4f}, Loss: {test_loss:.4f}\")\n",
    "        \n",
    "        # Clear memory\n",
    "        del model, X_train, X_test, y_train, y_test\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"✗ Error training {symbol}: {e}\")\n",
    "        training_results[symbol] = {'error': str(e)}\n",
    "        \n",
    "        # Clear memory on error\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "    # Progress update\n",
    "    elapsed_time = datetime.now() - training_start_time\n",
    "    avg_time_per_stock = elapsed_time / (i + 1)\n",
    "    remaining_stocks = len(training_data) - (i + 1)\n",
    "    estimated_remaining = avg_time_per_stock * remaining_stocks\n",
    "    \n",
    "    print(f\"Progress: {i+1}/{len(training_data)} ({(i+1)/len(training_data)*100:.1f}%)\")\n",
    "    print(f\"Elapsed: {elapsed_time}, Estimated remaining: {estimated_remaining}\")\n",
    "\n",
    "training_end_time = datetime.now()\n",
    "total_training_time = training_end_time - training_start_time\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"TRAINING COMPLETED!\")\n",
    "print(f\"{'='*60}\")\n",
    "print(f\"Total time: {total_training_time}\")\n",
    "print(f\"Successfully trained: {len([r for r in training_results.values() if 'error' not in r])} models\")\n",
    "print(f\"Failed: {len([r for r in training_results.values() if 'error' in r])} models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Save Training Results\n",
    "import json\n",
    "\n",
    "# Prepare summary\n",
    "summary = {\n",
    "    'training_start': training_start_time.isoformat(),\n",
    "    'training_end': training_end_time.isoformat(),\n",
    "    'total_duration': str(total_training_time),\n",
    "    'total_stocks': len(training_data),\n",
    "    'successful_models': len([r for r in training_results.values() if 'error' not in r]),\n",
    "    'failed_models': len([r for r in training_results.values() if 'error' in r]),\n",
    "    'training_config': TRAINING_CONFIG,\n",
    "    'gpu_info': gpu_name,\n",
    "    'results': training_results\n",
    "}\n",
    "\n",
    "# Save to Google Drive\n",
    "results_file = os.path.join(logs_path, f'lstm_training_results_{training_start_time.strftime(\"%Y%m%d_%H%M%S\")}.json')\n",
    "with open(results_file, 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"Training results saved to: {results_file}\")\n",
    "\n",
    "# Display summary statistics\n",
    "successful_results = [r for r in training_results.values() if 'error' not in r]\n",
    "if successful_results:\n",
    "    accuracies = [r['test_accuracy'] for r in successful_results]\n",
    "    print(f\"\\nAccuracy Statistics:\")\n",
    "    print(f\"Mean: {np.mean(accuracies):.4f}\")\n",
    "    print(f\"Std: {np.std(accuracies):.4f}\")\n",
    "    print(f\"Min: {np.min(accuracies):.4f}\")\n",
    "    print(f\"Max: {np.max(accuracies):.4f}\")\n",
    "\n",
    "print(f\"\\nAll models saved to: {models_path}\")\n",
    "print(f\"Training completed successfully! 🎉\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}